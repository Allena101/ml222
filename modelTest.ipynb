{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 15)\n",
      "(1000, 1)\n",
      "(1000, 15)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"X_train_tran.csv\")\n",
    "y_train = pd.read_csv(\"y_train_tran.csv\")\n",
    "y_train = np.ravel(y_train)\n",
    "X_val = pd.read_csv(\"X_val_tran.csv\")\n",
    "y_val = pd.read_csv(\"y_val_tran.csv\")\n",
    "myColumns = X_train.columns\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "df = pd.read_csv(\"Customer-Churn-Records.csv\")\n",
    "df = df.drop(columns=['RowNumber','CustomerId'])\n",
    "df = df.drop(columns=['Complain'])\n",
    "df = df.rename(columns={'Card Type': 'CardType'})\n",
    "df = df.rename(columns={'Point Earned': 'PointsEarned'})\n",
    "df = df.rename(columns={'Geography': 'Country'})\n",
    "df = df.rename(columns={'Satisfaction Score': 'SatisfactionScore'})\n",
    "# save original columns\n",
    "myColumns = df.columns\n",
    "\n",
    "X = pd.read_csv(\"X.csv\")\n",
    "X_train_cluster = pd.read_csv(\"X_train_cluster.csv\")\n",
    "X_val_cluster = pd.read_csv(\"X_val_cluster.csv\")\n",
    "X_test_cluster = pd.read_csv(\"X_test_cluster.csv\")\n",
    "\n",
    "\n",
    "XX = df.drop(['Exited','Surname'], axis=1) # I drop Surname since we have not encoded it\n",
    "y = df['Exited']\n",
    "# yy = np.ravel(y)\n",
    "X_trainDF, X_testDF, y_trainDF, y_testDF = train_test_split(XX, y, test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Surname', 'CreditScore', 'Country', 'Gender', 'Age', 'Tenure',\n",
       "       'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember',\n",
       "       'EstimatedSalary', 'Exited', 'SatisfactionScore', 'CardType',\n",
       "       'PointsEarned'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 18)\n",
      "(8000,)\n",
      "(1000, 18)\n",
      "(1000,)\n",
      "(1000, 18)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split the dataset into training, temporary holdout, and final holdout sets\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(df, df['Exited'], test_size=0.2, stratify=df['Exited'], random_state=42)\n",
    "\n",
    "# # Split the temporary holdout set into validation and final holdout sets\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)\n",
    "# print(X_val.shape)\n",
    "# print(y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Random Forest\": RandomForestRegressor()\n",
    "\"Decision Tree\": DecisionTreeRegressor()\n",
    "\"Gradient Boosting\": GradientBoostingRegressor()\n",
    "\"Linear Regression\": LinearRegression()\n",
    "\"XGBRegressor\": XGBRegressor()\n",
    "\"CatBoosting Regressor\": CatBoostRegressor(verbose=False)\n",
    "\"AdaBoost Regressor\": AdaBoostRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 1)\n",
      "(8000,)\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best Score (F-1): 0.39348612904550817\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create an instance of GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring=make_scorer(f1_score, greater_is_better=False), verbose=2, n_jobs=-1)\n",
    "\n",
    "print(y_train.shape)\n",
    "y_train = np.ravel(y_train)\n",
    "print(y_train.shape)\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score (F-1):\", -grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Feature Importances:\n",
      "target__AgeOrd 0.16715487895636677\n",
      "target__Tenure 0.1478561817701612\n",
      "target__NumOfProducts 0.1429802655137449\n",
      "target__BalanceOrd 0.09961685910765535\n",
      "target__PointsEarnedQuant 0.09729210965645636\n",
      "target__SatisfactionScore 0.09558576791667456\n",
      "target__TaxBracket 0.07210813285378426\n",
      "target__CountryOrd 0.06217266480062761\n",
      "target__IsActiveMember 0.04325779157942778\n",
      "target__HasCrCard 0.032400015971643076\n",
      "target__GenderBinary 0.029748324247093123\n",
      "target__SurnameOrd 0.009827007626365078\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92       796\n",
      "           1       0.75      0.51      0.61       204\n",
      "\n",
      "    accuracy                           0.87      1000\n",
      "   macro avg       0.82      0.74      0.76      1000\n",
      "weighted avg       0.86      0.87      0.86      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# X_train_cat = X_train[['GenderBinary', 'CountryOrd','SurnameOrd','HasCrCard','IsActiveMember']]\n",
    "\n",
    "# X_train_num = X_train[['Tenure', 'NumOfProducts','Satisfaction Score', 'PointsEarnedQuant','AgeOrd','TaxBracket','BalanceOrd',]]\n",
    "\n",
    "target_enc_features = ['GenderBinary', 'CountryOrd','SurnameOrd','HasCrCard','IsActiveMember', 'Tenure', 'NumOfProducts','SatisfactionScore', 'PointsEarnedQuant','AgeOrd','TaxBracket','BalanceOrd']\n",
    "\n",
    "\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('target', TargetEncoder(), target_enc_features),\n",
    "    ])\n",
    "\n",
    "# Adjusts weights so that the undersampled class get higher weight value\n",
    "class_weights = dict(zip(np.unique(y_train), [len(y_train) / (len(np.unique(y_train)) * np.bincount(y_train)[i]) for i in np.unique(y_train)]))\n",
    "class_weights\n",
    "\n",
    "# Define the Random Forest classifier\n",
    "classifier = RandomForestClassifier(class_weight=class_weights)\n",
    "\n",
    "# Create the pipeline\n",
    "pipe_target_enc = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe_target_enc.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pipe_target_enc.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importances\n",
    "importance_df = pd.DataFrame({'Feature': pipe_target_enc.named_steps['preprocessor'].get_feature_names_out(), 'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance (descending order)\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(importance_df['Feature'], importance_df['Importance']):\n",
    "    print(feature, importance)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipe_target_enc.predict(X_val)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_val, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best Parameters: {'classifier__max_depth': 8, 'classifier__n_estimators': 350, 'preprocessor__target__smoothing': 1.0}\n",
      "Best Score (F1 Macro): 0.7474072980454554\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92      1616\n",
      "           1       0.71      0.54      0.61       384\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.80      0.74      0.77      2000\n",
      "weighted avg       0.86      0.87      0.86      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'preprocessor__target__smoothing': [1.0,2, 5, 7],\n",
    "    'classifier__n_estimators': [250, 300, 350],\n",
    "    'classifier__max_depth': [6,7,8,9]\n",
    "}\n",
    "\n",
    "# pipe_target_enc\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(pipe_onehot_targetEnc, param_grid, cv=5, scoring='f1_macro', verbose=4, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "grid_search.fit(X_trainDF, y_trainDF)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score (F1 Macro):\", grid_search.best_score_)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = best_model.predict(X_testDF)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_testDF, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
    "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
    "Best Parameters: {'classifier__max_depth': 9, 'classifier__n_estimators': 250, 'preprocessor__target__smoothing': 7}\n",
    "Best Score (F1 Macro): 0.7433245279184252\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      0.85      0.88       796\n",
    "           1       0.55      0.71      0.62       204\n",
    "\n",
    "    accuracy                           0.82      1000\n",
    "   macro avg       0.73      0.78      0.75      1000\n",
    "weighted avg       0.84      0.82      0.83      1000\n",
    "\n",
    "#! RandomForrest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "scaler__Age 0.2191045913194815\n",
      "target__NumOfProducts 0.1100858308924759\n",
      "scaler__Balance 0.10235206891549274\n",
      "scaler__PointsEarned 0.09242691657510439\n",
      "scaler__EstimatedSalary 0.09153805606020109\n",
      "scaler__CreditScore 0.09128327977506823\n",
      "target__Tenure 0.06039964749107905\n",
      "target__SatisfactionScore 0.040926670634999426\n",
      "target__CardType 0.020983991606153003\n",
      "target__Country 0.01916544736596254\n",
      "onehot__IsActiveMember_1 0.015067335357439786\n",
      "onehot__Country_Germany 0.013287458799656073\n",
      "onehot__IsActiveMember_0 0.013049338055167531\n",
      "target__IsActiveMember 0.01149530452115938\n",
      "onehot__CardType_SILVER 0.010055495622611814\n",
      "onehot__CardType_PLATINUM 0.009360755792184072\n",
      "onehot__Country_France 0.008683822533543136\n",
      "onehot__CardType_DIAMOND 0.008604852618898741\n",
      "target__HasCrCard 0.008329768540221065\n",
      "onehot__HasCrCard_1 0.008246432219189644\n",
      "onehot__CardType_GOLD 0.008220944863335305\n",
      "onehot__HasCrCard_0 0.007864344691387418\n",
      "target__Gender 0.007723068337277875\n",
      "onehot__Gender_Female 0.0076480085922276415\n",
      "onehot__Gender_Male 0.007278293402827445\n",
      "onehot__Country_Spain 0.0068182754168551385\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92      1616\n",
      "           1       0.78      0.45      0.57       384\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.83      0.71      0.75      2000\n",
      "weighted avg       0.86      0.87      0.86      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the categorical features for one-hot encoding\n",
    "categorical_features = ['Country', 'Gender', 'HasCrCard', 'IsActiveMember','CardType']\n",
    "\n",
    "# Define the ordinal features for ordinal encoding\n",
    "ordinal_features = ['Tenure', 'NumOfProducts', 'SatisfactionScore']\n",
    "\n",
    "# Define the continuous features for scaling\n",
    "continuous_features = ['CreditScore', 'Age', 'Balance', 'PointsEarned', 'EstimatedSalary']\n",
    "\n",
    "target_enc_features = ['Tenure', 'NumOfProducts', 'SatisfactionScore', 'Country', 'Gender', 'HasCrCard', 'IsActiveMember','CardType']\n",
    "\n",
    "\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(), categorical_features),\n",
    "        ('target', TargetEncoder(), target_enc_features),\n",
    "        ('scaler', RobustScaler(), continuous_features) # For potential outliers\n",
    "    ])\n",
    "\n",
    "# Adjusts weights so that the undersampled class get higher weight\n",
    "class_weights = dict(zip(np.unique(y_trainDF), [len(y_trainDF) / (len(np.unique(y_trainDF)) * np.bincount(y_trainDF)[i]) for i in np.unique(y_trainDF)]))\n",
    "class_weights\n",
    "\n",
    "# Define the Random Forest classifier\n",
    "classifier = RandomForestClassifier(class_weight=class_weights)\n",
    "\n",
    "# Create the pipeline\n",
    "pipe_onehot_targetEnc = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe_onehot_targetEnc.fit(X_trainDF, y_trainDF)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pipe_onehot_targetEnc.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importances\n",
    "importance_df = pd.DataFrame({'Feature': pipe_onehot_targetEnc.named_steps['preprocessor'].get_feature_names_out(), 'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance (descending order)\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(importance_df['Feature'], importance_df['Importance']):\n",
    "    print(feature, importance)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipe_onehot_targetEnc.predict(X_testDF)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_testDF, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LightGBM model\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('onehot', OneHotEncoder(), categorical_features),\n",
    "#         ('target', TargetEncoder(), target_enc_features),\n",
    "#         ('scaler', RobustScaler(), continuous_features) # For potential outliers\n",
    "#     ])\n",
    "\n",
    "\n",
    "lgm = lgb.LGBMClassifier()\n",
    "pipe_target_enc.named_steps['classifier'] = lgm\n",
    "# pipe_onehot_targetEnc = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('classifier', model)\n",
    "# ])\n",
    "\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    # 'num_leaves': [10, 20, 30],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "# Define the scoring metric\n",
    "scoring = 'accuracy'  # Change to your desired scoring metric\n",
    "\n",
    "# Perform GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(pipe_target_enc, param_grid, scoring=scoring, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_val)\n",
    "classification_report = classification_report(y_val, y_pred)\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 7, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "Best Score (F1 Macro): 0.7037421002959607\n",
      "RandomForestClassifier\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.99      0.93       796\n",
      "           1       0.89      0.44      0.59       204\n",
      "\n",
      "    accuracy                           0.87      1000\n",
      "   macro avg       0.88      0.71      0.76      1000\n",
      "weighted avg       0.88      0.87      0.86      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the parameter values\n",
    "criteria = ['gini', 'entropy']\n",
    "max_features = ['sqrt', 'log2']\n",
    "max_depth = [3, 5, 7]\n",
    "min_samples_split = [2, 4, 6]\n",
    "min_samples_leaf = [1, 2, 3]\n",
    "\n",
    "param = {\n",
    "    'criterion': criteria,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf\n",
    "}\n",
    "\n",
    "# Create an instance of the RandomForestClassifier\n",
    "rf_cls = RandomForestClassifier()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(rf_cls, param, cv=5, scoring='f1_macro', verbose=4, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score (F1 Macro):\", grid_search.best_score_)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Use the best model for prediction\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_val, y_pred)\n",
    "print('Model:  ' rf_cls.__class__.__name__)\n",
    "print()\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Tangerine'> \n",
    "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
    "Best Parameters: {'criterion': 'gini', 'max_depth': 7, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
    "Best Score (F1 Macro): 0.7037421002959607\n",
    "Model name:  RandomForestClassifier\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.87      0.99      0.93       796\n",
    "           1       0.89      0.44      0.59       204\n",
    "\n",
    "    accuracy                           0.87      1000\n",
    "   macro avg       0.88      0.71      0.76      1000\n",
    "weighted avg       0.88      0.87      0.86      1000\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__num_leaves': 30}\n",
      "Best Score (F1 Macro): 0.7286351717896473\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.79      0.86       796\n",
      "           1       0.48      0.76      0.59       204\n",
      "\n",
      "    accuracy                           0.79      1000\n",
      "   macro avg       0.71      0.78      0.72      1000\n",
      "weighted avg       0.84      0.79      0.80      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "target_enc_features = ['GenderBinary', 'CountryOrd', 'SurnameOrd', 'HasCrCard', 'IsActiveMember', 'Tenure', 'NumOfProducts',\n",
    "                       'SatisfactionScore', 'PointsEarnedQuant', 'AgeOrd', 'TaxBracket', 'BalanceOrd']\n",
    "\n",
    "\n",
    "# Increase the weight value in proportion to its frequency to balance a bit for the current undersampling\n",
    "class_weights = dict(zip(np.unique(y_train), [len(y_train) / (len(np.unique(y_train)) * np.bincount(y_train)[i]) for i in np.unique(y_train)]))\n",
    "\n",
    "\n",
    "# Create an instance of the LGBMClassifier\n",
    "lgm = lgb.LGBMClassifier(class_weight=class_weights)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'classifier__num_leaves': [20,25,30],\n",
    "    'classifier__max_depth': [7,8,9],\n",
    "    'classifier__learning_rate': [0.05, 0.1,0.2,0.25]\n",
    "}\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('target_encoder', TargetEncoder(), target_enc_features),\n",
    "])\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline2 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lgm)\n",
    "])\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline2, param_grid, cv=5, scoring='f1_macro', verbose=4, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score (F1 Macro):\", grid_search.best_score_)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "report = classification_report(y_val, y_pred)\n",
    "print()\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Tangerine'> \n",
    "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
    "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
    "Best Parameters: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__num_leaves': 20}\n",
    "Best Score (F1 Macro): 0.7398664837604242\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.89      0.96      0.92       796\n",
    "           1       0.78      0.54      0.64       204\n",
    "\n",
    "    accuracy                           0.88      1000\n",
    "   macro avg       0.84      0.75      0.78      1000\n",
    "weighted avg       0.87      0.88      0.87      1000\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'classifier__C': 1.0, 'classifier__penalty': 'l2'}\n",
      "Best Score (F1 Macro): 0.6389185818926236\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.69      0.78       796\n",
      "           1       0.36      0.70      0.48       204\n",
      "\n",
      "    accuracy                           0.69      1000\n",
      "   macro avg       0.63      0.69      0.63      1000\n",
      "weighted avg       0.79      0.69      0.72      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_enc_features = ['GenderBinary', 'CountryOrd', 'SurnameOrd', 'HasCrCard', 'IsActiveMember', 'Tenure', 'NumOfProducts',\n",
    "                       'SatisfactionScore', 'PointsEarnedQuant', 'AgeOrd', 'TaxBracket', 'BalanceOrd']\n",
    "\n",
    "\n",
    "# Increase the weight value in proportion to its frequency to balance a bit for the current undersampling\n",
    "class_weights = dict(zip(np.unique(y_train), [len(y_train) / (len(np.unique(y_train)) * np.bincount(y_train)[i]) for i in np.unique(y_train)]))\n",
    "\n",
    "\n",
    "# Create an instance of the LGBMClassifier\n",
    "logreg = LogisticRegression(class_weight=class_weights) # class_weight=class_weights\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1.0, 10.0],\n",
    "    'classifier__penalty': ['l2']\n",
    "}\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('target_encoder', TargetEncoder(), target_enc_features),\n",
    "])\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipe_logReg = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', logreg)\n",
    "])\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(pipe_logReg, param_grid, cv=5, scoring='f1_macro', verbose=4, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score (F1 Macro):\", grid_search.best_score_)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "report = classification_report(y_val, y_pred)\n",
    "print()\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Tangerine'> \n",
    "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
    "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
    "Best Parameters: {'classifier__C': 1.0, 'classifier__penalty': 'l2'}\n",
    "Best Score (F1 Macro): 0.6389185818926236\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      0.71      0.80       796\n",
    "           1       0.40      0.75      0.52       204\n",
    "\n",
    "    accuracy                           0.72      1000\n",
    "   macro avg       0.66      0.73      0.66      1000\n",
    "weighted avg       0.81      0.72      0.74      1000\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 36 candidates, totalling 252 fits\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'classifier__learning_rate': 0.05, 'classifier__max_depth': 7, 'classifier__n_estimators': 200}\n",
      "Best Score (F1 Macro): 0.7387595084567579\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91       796\n",
      "           1       0.71      0.45      0.55       204\n",
      "\n",
      "    accuracy                           0.85      1000\n",
      "   macro avg       0.79      0.70      0.73      1000\n",
      "weighted avg       0.84      0.85      0.84      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_enc_features = ['GenderBinary', 'CountryOrd', 'SurnameOrd', 'HasCrCard', 'IsActiveMember', 'Tenure', 'NumOfProducts',\n",
    "                       'SatisfactionScore', 'PointsEarnedQuant', 'AgeOrd', 'TaxBracket', 'BalanceOrd']\n",
    "\n",
    "# Increase the weight value in proportion to its frequency to balance a bit for the current undersampling\n",
    "class_weights = dict(zip(np.unique(y_train), [len(y_train) / (len(np.unique(y_train)) * np.bincount(y_train)[i]) for i in np.unique(y_train)]))\n",
    "\n",
    "# Create an instance of the XGBClassifier\n",
    "xgb_cls = XGBClassifier()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'classifier__learning_rate': [0.05, 0.1, 0.15],\n",
    "    'classifier__max_depth': [3, 5, 7,9],\n",
    "    'classifier__n_estimators': [150, 200, 300]\n",
    "}\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('target_encoder', TargetEncoder(), target_enc_features),\n",
    "])\n",
    "\n",
    "# Create the pipeline\n",
    "pipe_xgb_cls = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', xgb_cls)\n",
    "])\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(pipe_xgb_cls, param_grid, cv=7, scoring='f1_macro', verbose=4, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score (F1 Macro):\", grid_search.best_score_)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "report = classification_report(y_val, y_pred)\n",
    "print()\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Tangerine'> \n",
    "Fitting 7 folds for each of 36 candidates, totalling 252 fits\n",
    "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
    "Best Parameters: {'classifier__learning_rate': 0.05, 'classifier__max_depth': 7, 'classifier__n_estimators': 200}\n",
    "Best Score (F1 Macro): 0.7387595084567579\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.89      0.96      0.92       796\n",
    "           1       0.76      0.53      0.63       204\n",
    "\n",
    "    accuracy                           0.87      1000\n",
    "   macro avg       0.83      0.75      0.78      1000\n",
    "weighted avg       0.86      0.87      0.86      1000\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 36 candidates, totalling 252 fits\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "0:\tlearn: 0.6216110\ttotal: 56.2ms\tremaining: 8.37s\n",
      "1:\tlearn: 0.5657334\ttotal: 58.3ms\tremaining: 4.32s\n",
      "2:\tlearn: 0.5240311\ttotal: 60.5ms\tremaining: 2.97s\n",
      "3:\tlearn: 0.4927936\ttotal: 62.6ms\tremaining: 2.28s\n",
      "4:\tlearn: 0.4672428\ttotal: 64.8ms\tremaining: 1.88s\n",
      "5:\tlearn: 0.4463725\ttotal: 68.4ms\tremaining: 1.64s\n",
      "6:\tlearn: 0.4290862\ttotal: 70.7ms\tremaining: 1.45s\n",
      "7:\tlearn: 0.4162892\ttotal: 73ms\tremaining: 1.29s\n",
      "8:\tlearn: 0.4040249\ttotal: 75.1ms\tremaining: 1.18s\n",
      "9:\tlearn: 0.3944090\ttotal: 77.6ms\tremaining: 1.09s\n",
      "10:\tlearn: 0.3848699\ttotal: 79.8ms\tremaining: 1.01s\n",
      "11:\tlearn: 0.3793010\ttotal: 81.9ms\tremaining: 942ms\n",
      "12:\tlearn: 0.3737354\ttotal: 84.5ms\tremaining: 891ms\n",
      "13:\tlearn: 0.3693152\ttotal: 87.6ms\tremaining: 851ms\n",
      "14:\tlearn: 0.3648079\ttotal: 89.8ms\tremaining: 808ms\n",
      "15:\tlearn: 0.3607570\ttotal: 92.3ms\tremaining: 773ms\n",
      "16:\tlearn: 0.3591487\ttotal: 93.8ms\tremaining: 734ms\n",
      "17:\tlearn: 0.3565105\ttotal: 96.2ms\tremaining: 705ms\n",
      "18:\tlearn: 0.3543571\ttotal: 99.2ms\tremaining: 684ms\n",
      "19:\tlearn: 0.3513683\ttotal: 101ms\tremaining: 660ms\n",
      "20:\tlearn: 0.3493525\ttotal: 104ms\tremaining: 636ms\n",
      "21:\tlearn: 0.3469239\ttotal: 107ms\tremaining: 622ms\n",
      "22:\tlearn: 0.3447824\ttotal: 109ms\tremaining: 604ms\n",
      "23:\tlearn: 0.3426912\ttotal: 111ms\tremaining: 585ms\n",
      "24:\tlearn: 0.3417281\ttotal: 114ms\tremaining: 569ms\n",
      "25:\tlearn: 0.3404211\ttotal: 117ms\tremaining: 557ms\n",
      "26:\tlearn: 0.3391999\ttotal: 119ms\tremaining: 543ms\n",
      "27:\tlearn: 0.3377625\ttotal: 122ms\tremaining: 530ms\n",
      "28:\tlearn: 0.3365379\ttotal: 123ms\tremaining: 515ms\n",
      "29:\tlearn: 0.3358001\ttotal: 126ms\tremaining: 503ms\n",
      "30:\tlearn: 0.3348871\ttotal: 128ms\tremaining: 490ms\n",
      "31:\tlearn: 0.3339986\ttotal: 130ms\tremaining: 479ms\n",
      "32:\tlearn: 0.3332689\ttotal: 132ms\tremaining: 468ms\n",
      "33:\tlearn: 0.3331340\ttotal: 134ms\tremaining: 456ms\n",
      "34:\tlearn: 0.3324336\ttotal: 136ms\tremaining: 446ms\n",
      "35:\tlearn: 0.3317450\ttotal: 138ms\tremaining: 436ms\n",
      "36:\tlearn: 0.3310642\ttotal: 140ms\tremaining: 427ms\n",
      "37:\tlearn: 0.3296796\ttotal: 142ms\tremaining: 419ms\n",
      "38:\tlearn: 0.3287934\ttotal: 144ms\tremaining: 410ms\n",
      "39:\tlearn: 0.3282291\ttotal: 147ms\tremaining: 403ms\n",
      "40:\tlearn: 0.3272568\ttotal: 149ms\tremaining: 397ms\n",
      "41:\tlearn: 0.3265155\ttotal: 151ms\tremaining: 389ms\n",
      "42:\tlearn: 0.3260685\ttotal: 154ms\tremaining: 382ms\n",
      "43:\tlearn: 0.3259900\ttotal: 155ms\tremaining: 373ms\n",
      "44:\tlearn: 0.3255028\ttotal: 157ms\tremaining: 366ms\n",
      "45:\tlearn: 0.3248845\ttotal: 159ms\tremaining: 359ms\n",
      "46:\tlearn: 0.3243635\ttotal: 163ms\tremaining: 357ms\n",
      "47:\tlearn: 0.3232833\ttotal: 165ms\tremaining: 351ms\n",
      "48:\tlearn: 0.3227271\ttotal: 167ms\tremaining: 345ms\n",
      "49:\tlearn: 0.3226621\ttotal: 168ms\tremaining: 337ms\n",
      "50:\tlearn: 0.3221663\ttotal: 171ms\tremaining: 332ms\n",
      "51:\tlearn: 0.3217211\ttotal: 173ms\tremaining: 327ms\n",
      "52:\tlearn: 0.3210489\ttotal: 176ms\tremaining: 322ms\n",
      "53:\tlearn: 0.3206436\ttotal: 178ms\tremaining: 317ms\n",
      "54:\tlearn: 0.3199070\ttotal: 181ms\tremaining: 313ms\n",
      "55:\tlearn: 0.3193074\ttotal: 185ms\tremaining: 310ms\n",
      "56:\tlearn: 0.3192046\ttotal: 186ms\tremaining: 304ms\n",
      "57:\tlearn: 0.3181088\ttotal: 188ms\tremaining: 299ms\n",
      "58:\tlearn: 0.3177053\ttotal: 191ms\tremaining: 295ms\n",
      "59:\tlearn: 0.3169997\ttotal: 194ms\tremaining: 290ms\n",
      "60:\tlearn: 0.3164359\ttotal: 197ms\tremaining: 287ms\n",
      "61:\tlearn: 0.3160413\ttotal: 200ms\tremaining: 284ms\n",
      "62:\tlearn: 0.3156003\ttotal: 202ms\tremaining: 279ms\n",
      "63:\tlearn: 0.3151496\ttotal: 204ms\tremaining: 274ms\n",
      "64:\tlearn: 0.3144559\ttotal: 206ms\tremaining: 270ms\n",
      "65:\tlearn: 0.3137982\ttotal: 209ms\tremaining: 267ms\n",
      "66:\tlearn: 0.3131939\ttotal: 211ms\tremaining: 262ms\n",
      "67:\tlearn: 0.3126195\ttotal: 214ms\tremaining: 258ms\n",
      "68:\tlearn: 0.3123964\ttotal: 216ms\tremaining: 254ms\n",
      "69:\tlearn: 0.3116917\ttotal: 218ms\tremaining: 250ms\n",
      "70:\tlearn: 0.3110109\ttotal: 220ms\tremaining: 245ms\n",
      "71:\tlearn: 0.3104543\ttotal: 223ms\tremaining: 241ms\n",
      "72:\tlearn: 0.3096998\ttotal: 225ms\tremaining: 237ms\n",
      "73:\tlearn: 0.3093994\ttotal: 227ms\tremaining: 233ms\n",
      "74:\tlearn: 0.3091056\ttotal: 229ms\tremaining: 229ms\n",
      "75:\tlearn: 0.3086071\ttotal: 231ms\tremaining: 225ms\n",
      "76:\tlearn: 0.3083621\ttotal: 233ms\tremaining: 221ms\n",
      "77:\tlearn: 0.3078582\ttotal: 235ms\tremaining: 217ms\n",
      "78:\tlearn: 0.3074195\ttotal: 237ms\tremaining: 213ms\n",
      "79:\tlearn: 0.3071689\ttotal: 240ms\tremaining: 210ms\n",
      "80:\tlearn: 0.3068851\ttotal: 242ms\tremaining: 206ms\n",
      "81:\tlearn: 0.3066345\ttotal: 244ms\tremaining: 202ms\n",
      "82:\tlearn: 0.3063702\ttotal: 246ms\tremaining: 199ms\n",
      "83:\tlearn: 0.3059464\ttotal: 248ms\tremaining: 195ms\n",
      "84:\tlearn: 0.3057280\ttotal: 250ms\tremaining: 191ms\n",
      "85:\tlearn: 0.3051036\ttotal: 252ms\tremaining: 188ms\n",
      "86:\tlearn: 0.3048087\ttotal: 255ms\tremaining: 185ms\n",
      "87:\tlearn: 0.3044902\ttotal: 257ms\tremaining: 181ms\n",
      "88:\tlearn: 0.3041563\ttotal: 259ms\tremaining: 178ms\n",
      "89:\tlearn: 0.3038558\ttotal: 261ms\tremaining: 174ms\n",
      "90:\tlearn: 0.3034013\ttotal: 264ms\tremaining: 171ms\n",
      "91:\tlearn: 0.3029787\ttotal: 266ms\tremaining: 168ms\n",
      "92:\tlearn: 0.3026389\ttotal: 269ms\tremaining: 165ms\n",
      "93:\tlearn: 0.3021766\ttotal: 272ms\tremaining: 162ms\n",
      "94:\tlearn: 0.3018987\ttotal: 274ms\tremaining: 159ms\n",
      "95:\tlearn: 0.3014319\ttotal: 277ms\tremaining: 156ms\n",
      "96:\tlearn: 0.3010378\ttotal: 279ms\tremaining: 152ms\n",
      "97:\tlearn: 0.3007063\ttotal: 282ms\tremaining: 150ms\n",
      "98:\tlearn: 0.3003006\ttotal: 286ms\tremaining: 147ms\n",
      "99:\tlearn: 0.2999582\ttotal: 288ms\tremaining: 144ms\n",
      "100:\tlearn: 0.2996886\ttotal: 290ms\tremaining: 141ms\n",
      "101:\tlearn: 0.2993424\ttotal: 292ms\tremaining: 137ms\n",
      "102:\tlearn: 0.2988984\ttotal: 294ms\tremaining: 134ms\n",
      "103:\tlearn: 0.2986275\ttotal: 296ms\tremaining: 131ms\n",
      "104:\tlearn: 0.2983478\ttotal: 298ms\tremaining: 128ms\n",
      "105:\tlearn: 0.2979035\ttotal: 302ms\tremaining: 125ms\n",
      "106:\tlearn: 0.2977268\ttotal: 305ms\tremaining: 123ms\n",
      "107:\tlearn: 0.2974502\ttotal: 307ms\tremaining: 119ms\n",
      "108:\tlearn: 0.2969930\ttotal: 309ms\tremaining: 116ms\n",
      "109:\tlearn: 0.2967570\ttotal: 312ms\tremaining: 113ms\n",
      "110:\tlearn: 0.2963783\ttotal: 314ms\tremaining: 110ms\n",
      "111:\tlearn: 0.2960956\ttotal: 318ms\tremaining: 108ms\n",
      "112:\tlearn: 0.2958836\ttotal: 321ms\tremaining: 105ms\n",
      "113:\tlearn: 0.2955787\ttotal: 323ms\tremaining: 102ms\n",
      "114:\tlearn: 0.2953885\ttotal: 325ms\tremaining: 99.1ms\n",
      "115:\tlearn: 0.2951541\ttotal: 328ms\tremaining: 96ms\n",
      "116:\tlearn: 0.2947871\ttotal: 330ms\tremaining: 93.1ms\n",
      "117:\tlearn: 0.2944371\ttotal: 333ms\tremaining: 90.2ms\n",
      "118:\tlearn: 0.2940637\ttotal: 335ms\tremaining: 87.3ms\n",
      "119:\tlearn: 0.2937650\ttotal: 339ms\tremaining: 84.7ms\n",
      "120:\tlearn: 0.2936221\ttotal: 341ms\tremaining: 81.8ms\n",
      "121:\tlearn: 0.2932606\ttotal: 344ms\tremaining: 78.9ms\n",
      "122:\tlearn: 0.2929897\ttotal: 348ms\tremaining: 76.3ms\n",
      "123:\tlearn: 0.2925645\ttotal: 350ms\tremaining: 73.4ms\n",
      "124:\tlearn: 0.2922350\ttotal: 352ms\tremaining: 70.5ms\n",
      "125:\tlearn: 0.2917549\ttotal: 355ms\tremaining: 67.5ms\n",
      "126:\tlearn: 0.2914666\ttotal: 357ms\tremaining: 64.7ms\n",
      "127:\tlearn: 0.2910190\ttotal: 359ms\tremaining: 61.8ms\n",
      "128:\tlearn: 0.2907823\ttotal: 362ms\tremaining: 59ms\n",
      "129:\tlearn: 0.2902737\ttotal: 367ms\tremaining: 56.5ms\n",
      "130:\tlearn: 0.2901986\ttotal: 371ms\tremaining: 53.8ms\n",
      "131:\tlearn: 0.2899719\ttotal: 374ms\tremaining: 51ms\n",
      "132:\tlearn: 0.2895703\ttotal: 376ms\tremaining: 48.1ms\n",
      "133:\tlearn: 0.2892688\ttotal: 379ms\tremaining: 45.3ms\n",
      "134:\tlearn: 0.2887820\ttotal: 381ms\tremaining: 42.4ms\n",
      "135:\tlearn: 0.2883263\ttotal: 384ms\tremaining: 39.5ms\n",
      "136:\tlearn: 0.2879006\ttotal: 386ms\tremaining: 36.7ms\n",
      "137:\tlearn: 0.2875313\ttotal: 389ms\tremaining: 33.8ms\n",
      "138:\tlearn: 0.2873303\ttotal: 391ms\tremaining: 30.9ms\n",
      "139:\tlearn: 0.2870573\ttotal: 394ms\tremaining: 28.1ms\n",
      "140:\tlearn: 0.2866893\ttotal: 396ms\tremaining: 25.3ms\n",
      "141:\tlearn: 0.2863010\ttotal: 398ms\tremaining: 22.4ms\n",
      "142:\tlearn: 0.2860525\ttotal: 400ms\tremaining: 19.6ms\n",
      "143:\tlearn: 0.2859510\ttotal: 402ms\tremaining: 16.8ms\n",
      "144:\tlearn: 0.2856790\ttotal: 405ms\tremaining: 14ms\n",
      "145:\tlearn: 0.2854399\ttotal: 411ms\tremaining: 11.2ms\n",
      "146:\tlearn: 0.2851502\ttotal: 414ms\tremaining: 8.44ms\n",
      "147:\tlearn: 0.2847676\ttotal: 416ms\tremaining: 5.62ms\n",
      "148:\tlearn: 0.2843533\ttotal: 419ms\tremaining: 2.81ms\n",
      "149:\tlearn: 0.2841590\ttotal: 421ms\tremaining: 0us\n",
      "Best Parameters: {'classifier__depth': 7, 'classifier__learning_rate': 0.15, 'classifier__n_estimators': 150}\n",
      "Best Score (F1 Macro): 0.7417607832161796\n",
      "CatBoostClassifier\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91       796\n",
      "           1       0.70      0.46      0.56       204\n",
      "\n",
      "    accuracy                           0.85      1000\n",
      "   macro avg       0.79      0.71      0.73      1000\n",
      "weighted avg       0.84      0.85      0.84      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_enc_features = ['GenderBinary', 'CountryOrd', 'SurnameOrd', 'HasCrCard', 'IsActiveMember', 'Tenure', 'NumOfProducts',\n",
    "                       'SatisfactionScore', 'PointsEarnedQuant', 'AgeOrd', 'TaxBracket', 'BalanceOrd']\n",
    "\n",
    "# Increase the weight value in proportion to its frequency to balance a bit for the current undersampling\n",
    "class_weights = dict(zip(np.unique(y_train), [len(y_train) / (len(np.unique(y_train)) * np.bincount(y_train)[i]) for i in np.unique(y_train)]))\n",
    "\n",
    "# Create an instance of the CatBoostClassifier\n",
    "catboost_cls = CatBoostClassifier()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'classifier__learning_rate': [0.05, 0.1, 0.15],\n",
    "    'classifier__depth': [3, 5, 7, 9],\n",
    "    'classifier__n_estimators': [150, 200, 300]\n",
    "}\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('target_encoder', TargetEncoder(), target_enc_features),\n",
    "])\n",
    "\n",
    "# Create the pipeline\n",
    "pipe_catboost_cls = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', catboost_cls)\n",
    "])\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(pipe_catboost_cls, param_grid, cv=7, scoring='f1_macro', verbose=4, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score (F1 Macro):\", grid_search.best_score_)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "report = classification_report(y_val, y_pred)\n",
    "print(catboost_cls.__class__.__name__)\n",
    "print()\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Tangerine'> \n",
    "Best Parameters: {'classifier__depth': 7, 'classifier__learning_rate': 0.15, 'classifier__n_estimators': 150}\n",
    "Best Score (F1 Macro): 0.7417607832161796\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.89      0.96      0.92       796\n",
    "           1       0.77      0.54      0.64       204\n",
    "\n",
    "    accuracy                           0.87      1000\n",
    "   macro avg       0.83      0.75      0.78      1000\n",
    "weighted avg       0.87      0.87      0.87      1000\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'categorical_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31860\\2017944254.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Magnus\\Desktop\\New\\newENV\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5985\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5986\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'categorical_features'"
     ]
    }
   ],
   "source": [
    "X.categorical_features.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 21)\n",
      "(1000, 21)\n",
      "(1000, 21)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_val_cluster.shape)\n",
    "print(X_test_cluster.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tenure</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>SatisfactionScore</th>\n",
       "      <th>GenderBinary</th>\n",
       "      <th>SurnameOrd</th>\n",
       "      <th>CardTypeOrd</th>\n",
       "      <th>CountryOrd</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>...</th>\n",
       "      <th>Balance</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>PointsEarned</th>\n",
       "      <th>TenurePoints</th>\n",
       "      <th>CardProducts</th>\n",
       "      <th>SatisfactionProducts</th>\n",
       "      <th>AgeProducts</th>\n",
       "      <th>BalanceCredit</th>\n",
       "      <th>ClusterKMeans</th>\n",
       "      <th>ClusterBGM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>104982.39</td>\n",
       "      <td>422</td>\n",
       "      <td>0.004739</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>744</td>\n",
       "      <td>...</td>\n",
       "      <td>43504.42</td>\n",
       "      <td>119327.75</td>\n",
       "      <td>607</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>58.473683</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>773</td>\n",
       "      <td>...</td>\n",
       "      <td>145578.28</td>\n",
       "      <td>186172.85</td>\n",
       "      <td>630</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>188.328952</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>646</td>\n",
       "      <td>...</td>\n",
       "      <td>105957.44</td>\n",
       "      <td>15470.91</td>\n",
       "      <td>345</td>\n",
       "      <td>0.011594</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>164.020805</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95463.29</td>\n",
       "      <td>632</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tenure  NumOfProducts  HasCrCard  IsActiveMember  SatisfactionScore  \\\n",
       "0       2              2          1               0                  2   \n",
       "1       1              1          1               1                  1   \n",
       "2       2              1          0               1                  1   \n",
       "3       4              1          1               0                  1   \n",
       "4       8              2          0               1                  3   \n",
       "\n",
       "   GenderBinary  SurnameOrd  CardTypeOrd  CountryOrd  CreditScore  ...  \\\n",
       "0             0         0.0          1.0           1          709  ...   \n",
       "1             0         0.0          2.0           2          744  ...   \n",
       "2             0         0.0          0.0           2          773  ...   \n",
       "3             1         2.0          2.0           3          646  ...   \n",
       "4             1         0.0          0.0           2          675  ...   \n",
       "\n",
       "     Balance  EstimatedSalary  PointsEarned  TenurePoints  CardProducts  \\\n",
       "0       0.00        104982.39           422      0.004739           0.5   \n",
       "1   43504.42        119327.75           607      0.001647           2.0   \n",
       "2  145578.28        186172.85           630      0.003175           0.0   \n",
       "3  105957.44         15470.91           345      0.011594           2.0   \n",
       "4       0.00         95463.29           632      0.012658           0.0   \n",
       "\n",
       "   SatisfactionProducts  AgeProducts  BalanceCredit  ClusterKMeans  ClusterBGM  \n",
       "0                   1.0         17.5       0.000000              1           3  \n",
       "1                   1.0         29.0      58.473683              0           2  \n",
       "2                   1.0         64.0     188.328952              0           2  \n",
       "3                   1.0         29.0     164.020805              0           2  \n",
       "4                   1.5         28.5       0.000000              1           3  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember',\n",
       "       'SatisfactionScore', 'GenderBinary', 'SurnameOrd', 'CardTypeOrd',\n",
       "       'CountryOrd', 'CreditScore', 'Age', 'Balance', 'EstimatedSalary',\n",
       "       'PointsEarned', 'TenurePoints', 'CardProducts', 'SatisfactionProducts',\n",
       "       'AgeProducts', 'BalanceCredit', 'ClusterKMeans', 'ClusterBGM'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(), categorical_features),\n",
    "        ('target', TargetEncoder(), target_enc_features),\n",
    "        ('scaler', RobustScaler(), continuous_features) # For potential outliers\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 36 candidates, totalling 252 fits\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 300}\n",
      "Best Score (F1 Macro): 0.750233730665283\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91       796\n",
      "           1       0.70      0.48      0.57       204\n",
      "\n",
      "    accuracy                           0.85      1000\n",
      "   macro avg       0.79      0.71      0.74      1000\n",
      "weighted avg       0.84      0.85      0.84      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "continuous_features = ['CreditScore', 'Age', 'Balance', 'NumOfProducts', 'EstimatedSalary', 'PointsEarned']\n",
    "\n",
    "categorical_features = ['HasCrCard', 'IsActiveMember', 'GenderBinary']\n",
    "\n",
    "cluster_features = ['ClusterKMeans', 'ClusterBGM']\n",
    "\n",
    "ordinal_features = ['Tenure', 'CardTypeOrd', 'NumOfProducts', 'NumOfProducts']\n",
    "\n",
    "target_enc_features = ['GenderBinary', 'CountryOrd', 'SurnameOrd', 'HasCrCard', 'IsActiveMember',  'Tenure', 'NumOfProducts','SatisfactionScore']\n",
    "\n",
    "# Increase the weight value in proportion to its frequency to balance a bit for the current undersampling\n",
    "class_weights = dict(zip(np.unique(y_train), [len(y_train) / (len(np.unique(y_train)) * np.bincount(y_train)[i]) for i in np.unique(y_train)]))\n",
    "\n",
    "# Create an instance of the XGBClassifier\n",
    "xgb_cls = XGBClassifier()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'classifier__learning_rate': [0.05, 0.1, 0.15],\n",
    "    'classifier__max_depth': [3, 5, 7,9],\n",
    "    'classifier__n_estimators': [150, 200, 300]\n",
    "}\n",
    "\n",
    "prep = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(), categorical_features),\n",
    "        ('target', TargetEncoder(), target_enc_features),\n",
    "        ('scaler', StandardScaler(), continuous_features),\n",
    "        ('ordinal', 'passthrough', ordinal_features),\n",
    "        ('cluster', 'passthrough', cluster_features),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipe_xgb_cls = Pipeline([\n",
    "    ('preprocessor', prep),\n",
    "    ('classifier', xgb_cls)\n",
    "])\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(pipe_xgb_cls, param_grid, cv=7, scoring='f1_macro', verbose=4, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score (F1 Macro):\", grid_search.best_score_)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_val_cluster)\n",
    "\n",
    "report = classification_report(y_val, y_pred)\n",
    "print()\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Tangerine'> \n",
    "Fitting 7 folds for each of 36 candidates, totalling 252 fits\n",
    "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
    "Best Parameters: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 300}\n",
    "Best Score (F1 Macro): 0.750233730665283\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.88      0.95      0.91       796\n",
    "           1       0.70      0.48      0.57       204\n",
    "\n",
    "    accuracy                           0.85      1000\n",
    "   macro avg       0.79      0.71      0.74      1000\n",
    "weighted avg       0.84      0.85      0.84      1000\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'classifier__max_depth': None, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 200}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91       796\n",
      "           1       0.75      0.46      0.57       204\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.81      0.71      0.74      1000\n",
      "weighted avg       0.85      0.86      0.84      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Define the preprocessing steps for different types of columns\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('onehot', OneHotEncoder(), ['HasCrCard', 'IsActiveMember', 'GenderBinary']),\n",
    "    ('target', TargetEncoder(), ['GenderBinary', 'CountryOrd', 'SurnameOrd', 'HasCrCard', 'IsActiveMember', 'Tenure', 'NumOfProducts', 'SatisfactionScore']),\n",
    "    ('scaler', StandardScaler(), ['CreditScore', 'Age', 'Balance', 'NumOfProducts', 'EstimatedSalary', 'PointsEarned']),\n",
    "    ('ordinal', 'passthrough', ['Tenure', 'CardTypeOrd', 'NumOfProducts', 'NumOfProducts']),\n",
    "    ('cluster', 'passthrough', ['ClusterKMeans', 'ClusterBGM'])\n",
    "])\n",
    "\n",
    "# Define the classifier\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Create the pipeline\n",
    "pipe_rf_cls = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Define the parameter grid\n",
    "param = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [None, 5, 10],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipe_rf_cls, param, cv=5, scoring='f1_macro', verbose=4, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Use the best model for prediction\n",
    "y_pred = best_model.predict(X_val_cluster)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_val, y_pred)\n",
    "print()\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Tangerine'> \n",
    "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
    "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
    "Best Parameters: {'classifier__max_depth': None, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 200}\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.87      0.96      0.91       796\n",
    "           1       0.75      0.46      0.57       204\n",
    "\n",
    "    accuracy                           0.86      1000\n",
    "   macro avg       0.81      0.71      0.74      1000\n",
    "weighted avg       0.85      0.86      0.84      1000\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember',\n",
       "       'SatisfactionScore', 'GenderBinary', 'SurnameOrd', 'CardTypeOrd',\n",
       "       'CountryOrd', 'CreditScore', 'Age', 'Balance', 'EstimatedSalary',\n",
       "       'PointsEarned', 'TenurePoints', 'CardProducts', 'SatisfactionProducts',\n",
       "       'AgeProducts', 'BalanceCredit', 'ClusterKMeans', 'ClusterBGM'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best Parameters: {'classifier__learning_rate': 0.05, 'classifier__max_depth': 5, 'classifier__n_estimators': 150}\n",
      "Best Score (F1 Macro): 0.7315665204230263\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91       796\n",
      "           1       0.75      0.45      0.56       204\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.81      0.70      0.74      1000\n",
      "weighted avg       0.85      0.86      0.84      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#* With semi duplicate features i.e. Balance AND BalanceCredit\n",
    "\n",
    "continuous_features = ['CreditScore', 'Age', 'Balance', 'NumOfProducts', 'EstimatedSalary', 'PointsEarned', 'TenurePoints', 'CardProducts', 'SatisfactionProducts', 'AgeProducts', 'BalanceCredit']\n",
    "\n",
    "categorical_features = ['HasCrCard', 'IsActiveMember', 'GenderBinary']\n",
    "\n",
    "cluster_features = ['ClusterKMeans', 'ClusterBGM']\n",
    "\n",
    "ordinal_features = ['Tenure', 'CardTypeOrd', 'NumOfProducts', 'NumOfProducts']\n",
    "\n",
    "target_enc_features = ['GenderBinary', 'CountryOrd', 'SurnameOrd', 'HasCrCard', 'IsActiveMember',  'Tenure', 'NumOfProducts','SatisfactionScore', 'TenurePoints', 'CardProducts', 'SatisfactionProducts', 'AgeProducts', 'BalanceCredit']\n",
    "\n",
    "# Increase the weight value in proportion to its frequency to balance a bit for the current undersampling\n",
    "class_weights = dict(zip(np.unique(y_train), [len(y_train) / (len(np.unique(y_train)) * np.bincount(y_train)[i]) for i in np.unique(y_train)]))\n",
    "\n",
    "# Create an instance of the XGBClassifier\n",
    "xgb_cls = XGBClassifier()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'classifier__learning_rate': [0.05, 0.1, 0.15],\n",
    "    'classifier__max_depth': [3, 5, 7,9],\n",
    "    'classifier__n_estimators': [150, 200, 300]\n",
    "}\n",
    "\n",
    "prep = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scaler', StandardScaler(), continuous_features),\n",
    "        ('ordinal', 'passthrough', ordinal_features),\n",
    "        ('cluster', 'passthrough', cluster_features),\n",
    "        ('categorical', 'passthrough', categorical_features)      \n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipe_xgb_cls = Pipeline([\n",
    "    ('preprocessor', prep),\n",
    "    ('classifier', xgb_cls)\n",
    "])\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search_xg = GridSearchCV(pipe_xgb_cls, param_grid, cv=3, scoring='f1_macro', verbose=4, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search_xg.fit(X, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search_xg.best_params_)\n",
    "print(\"Best Score (F1 Macro):\", grid_search_xg.best_score_)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search_xg.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_val_cluster)\n",
    "\n",
    "report = classification_report(y_val, y_pred)\n",
    "print()\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 300}\n",
      "Best Score (F1 Macro): 0.7385244777083596\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91       796\n",
      "           1       0.73      0.47      0.57       204\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.80      0.71      0.74      1000\n",
      "weighted avg       0.85      0.86      0.84      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#* Bummer. The feature engineering did not improve the resutls\n",
    "\n",
    "continuous_features = ['EstimatedSalary', 'TenurePoints', 'CardProducts', 'SatisfactionProducts', 'AgeProducts', 'BalanceCredit']\n",
    "\n",
    "categorical_features = ['HasCrCard', 'IsActiveMember', 'GenderBinary']\n",
    "\n",
    "cluster_features = ['ClusterKMeans', 'ClusterBGM']\n",
    "\n",
    "ordinal_features = []\n",
    "\n",
    "target_enc_features = ['CountryOrd', 'TenurePoints', 'SatisfactionProducts', 'AgeProducts', 'BalanceCredit']\n",
    "\n",
    "# Increase the weight value in proportion to its frequency to balance a bit for the current undersampling\n",
    "class_weights = dict(zip(np.unique(y_train), [len(y_train) / (len(np.unique(y_train)) * np.bincount(y_train)[i]) for i in np.unique(y_train)]))\n",
    "\n",
    "# Create an instance of the XGBClassifier\n",
    "xgb_cls = XGBClassifier()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'classifier__learning_rate': [0.05, 0.1, 0.15],\n",
    "    'classifier__max_depth': [3, 5, 7,9],\n",
    "    'classifier__n_estimators': [150, 200, 300]\n",
    "}\n",
    "\n",
    "prep = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # ('onehot', OneHotEncoder(), categorical_features),\n",
    "        ('target', TargetEncoder(), target_enc_features),\n",
    "        ('scaler', StandardScaler(), continuous_features),\n",
    "        ('ordinal', 'passthrough', ordinal_features),\n",
    "        ('cluster', 'passthrough', cluster_features),\n",
    "        ('categorical', 'passthrough', categorical_features)\n",
    "        \n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipe_xgb_cls = Pipeline([\n",
    "    ('preprocessor', prep),\n",
    "    ('classifier', xgb_cls)\n",
    "])\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search_xg222 = GridSearchCV(pipe_xgb_cls, param_grid, cv=3, scoring='f1_macro', verbose=4, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search_xg222.fit(X, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search_xg222.best_params_)\n",
    "print(\"Best Score (F1 Macro):\", grid_search_xg222.best_score_)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search_xg222.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_val_cluster)\n",
    "\n",
    "report = classification_report(y_val, y_pred)\n",
    "print()\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Tangerine'> \n",
    "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
    "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
    "Best Parameters: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 300}\n",
    "Best Score (F1 Macro): 0.7385244777083596\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.87      0.96      0.91       796\n",
    "           1       0.73      0.47      0.57       204\n",
    "\n",
    "    accuracy                           0.86      1000\n",
    "   macro avg       0.80      0.71      0.74      1000\n",
    "weighted avg       0.85      0.86      0.84      1000\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember',\n",
       "       'SatisfactionScore', 'GenderBinary', 'SurnameOrd', 'CardTypeOrd',\n",
       "       'CountryOrd', 'CreditScore', 'Age', 'Balance', 'EstimatedSalary',\n",
       "       'PointsEarned', 'TenurePoints', 'CardProducts', 'SatisfactionProducts',\n",
       "       'AgeProducts', 'BalanceCredit', 'ClusterKMeans', 'ClusterBGM'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_thresold = df['salary_in_usd'].quantile(0.98)\n",
    "max_thresold\n",
    "print(max_thresold) \n",
    "\n",
    "\n",
    "# df[df['salary_in_usd']>max_thresold]\n",
    "\n",
    "min_thresold = df['salary_in_usd'].quantile(0.03)\n",
    "min_thresold\n",
    "\n",
    "df[df['salary_in_usd']<min_thresold]\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "df = df[(df['salary_in_usd']<max_thresold) & (df['salary_in_usd']>min_thresold)]\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
